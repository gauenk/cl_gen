-=-=-=-=-=-=-=-=-=-=-=-
    "Good" Losses 
-=-=-=-=-=-=-=-=-=-=-=-

-=-=- KPN Filter Norm Loss -=-=-=-

Desc: The norm of each frame's set of filters is approximately equal
Motivation: We observe during training the relative norm of the filters for the non-centered frame decreases by 15% - 30%. This implies the model is ignoring the non-centered frames.

-=-=- Reconstruction Loss: MSE -=-=-

Desc: The MSE loss between each denoised frame and the centered noisy frame.
Motivation: Refined alignment and denoising of the images.
Issues: This causes blur in the reconstructed images.
Solution: We anneal the MSE loss during training.

-=-=- Reconstruction Loss: KL -=-=-

Desc: The KL loss between (a) each denoised frame's residual with the centered noisy frame and (b) a Gaussian distribution.
Motivation: Denoise the final image.

-=-=-=-=-=-=-=-=-=-=-=-
 "Problematic" Losses 
-=-=-=-=-=-=-=-=-=-=-=-

-=-=- Alignment Loss -=-=-

Desc: The MSE loss between the aligned frames and the centered noisy frame.
Motivation: Remove "large" movement the next model can not account for.
Issues: Causes blur in the aligned images. Removing this results in no meaningful learning of the KPN single frame model. 
Next Steps: Use the distribution loss for the "centered" images. The global motion means we can not properly account for edges in this model; the sides are cut-off. However, we can assume there is a middle patch we can align. 

-=-=- KPN-single-filter Within-Kernel Loss -=-=-

Desc: Penalize spread across the kernel; we want a single dot per kernel.
Motivation: We want to shift the image, not introduce blur.
Issue: This does not remove the blur from the alignment loss and may contribute to this model's inability to learn alignment.
Solution: See "alignment loss" + "kpn-single-filter across-batch loss" + more guessing

-=-=- KPN-single-filter Across-Batch Loss -=-=-

Desc: Penalize the model using the same kernels for each image in the batch;
Motivation: Each image in the batch uses different dynamics. Thus, each set of filters should not be the same.
Issue: This does not seem to help the model learn to produce different outputs
Solution: See "alignment loss"
